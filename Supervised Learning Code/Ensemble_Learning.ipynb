{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "FctM_0KS9ZWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmk9CPekCkyc",
        "outputId": "f8ba4ede-ac7a-44f7-dd80-47df0ac805a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "Confusion Matrix:\n",
            " [[19  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  0 13]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Simple Decision Tree for classification with max depth\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=3):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        n_samples, n_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "\n",
        "        # Stop criteria\n",
        "        if depth >= self.max_depth or num_classes == 1 or n_samples <= 1:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            return {'type': 'leaf', 'class': leaf_value}\n",
        "\n",
        "        # Find best split\n",
        "        best_feat, best_thresh, best_gain = None, None, 0\n",
        "        for feature in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for t in thresholds:\n",
        "                gain = self._information_gain(X[:, feature], y, t)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feat = feature\n",
        "                    best_thresh = t\n",
        "\n",
        "        if best_gain == 0:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            return {'type': 'leaf', 'class': leaf_value}\n",
        "\n",
        "        left_idx = X[:, best_feat] <= best_thresh\n",
        "        right_idx = X[:, best_feat] > best_thresh\n",
        "        left_tree = self._build_tree(X[left_idx], y[left_idx], depth+1)\n",
        "        right_tree = self._build_tree(X[right_idx], y[right_idx], depth+1)\n",
        "        return {'type': 'node', 'feature': best_feat, 'threshold': best_thresh,\n",
        "                'left': left_tree, 'right': right_tree}\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        counts = np.bincount(y)\n",
        "        probs = counts / len(y)\n",
        "        probs = probs[probs > 0]\n",
        "        return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "    def _information_gain(self, feature_col, y, threshold):\n",
        "        parent_entropy = self._entropy(y)\n",
        "        left_idx = feature_col <= threshold\n",
        "        right_idx = feature_col > threshold\n",
        "        n = len(y)\n",
        "        n_left, n_right = sum(left_idx), sum(right_idx)\n",
        "        if n_left == 0 or n_right == 0:\n",
        "            return 0\n",
        "        child_entropy = (n_left/n)*self._entropy(y[left_idx]) + (n_right/n)*self._entropy(y[right_idx])\n",
        "        return parent_entropy - child_entropy\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_sample(x, self.tree) for x in X])\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        if node['type'] == 'leaf':\n",
        "            return node['class']\n",
        "        if x[node['feature']] <= node['threshold']:\n",
        "            return self._predict_sample(x, node['left'])\n",
        "        else:\n",
        "            return self._predict_sample(x, node['right'])\n",
        "\n",
        "# Random Forest Classifier\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=10, max_features=None, max_depth=3):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        idx = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[idx], y[idx]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        n_features = X.shape[1]\n",
        "        max_features = self.max_features or int(np.sqrt(n_features))\n",
        "\n",
        "        for _ in range(self.n_trees):\n",
        "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
        "            feature_idx = np.random.choice(n_features, max_features, replace=False)\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample[:, feature_idx], y_sample)\n",
        "            tree.feature_idx_map = feature_idx\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = []\n",
        "        for tree in self.trees:\n",
        "            X_subset = X[:, tree.feature_idx_map]\n",
        "            tree_preds.append(tree.predict(X_subset))\n",
        "        tree_preds = np.array(tree_preds)\n",
        "        y_pred = []\n",
        "        for sample_preds in tree_preds.T:\n",
        "            vote = Counter(sample_preds).most_common(1)[0][0]\n",
        "            y_pred.append(vote)\n",
        "        return np.array(y_pred)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    data = load_iris()\n",
        "    X, y = data.data, data.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    rf = RandomForest(n_trees=15, max_features=2, max_depth=3)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaboost"
      ],
      "metadata": {
        "id": "Jt9p9GRO9WYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# -----------------------------\n",
        "# Simple Decision Stump (weak learner)\n",
        "# -----------------------------\n",
        "class DecisionStump:\n",
        "    def __init__(self):\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        self.polarity = 1\n",
        "        self.alpha = None\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        preds = np.ones(n_samples)\n",
        "        if self.polarity == 1:\n",
        "            preds[X[:, self.feature_index] <= self.threshold] = -1\n",
        "        else:\n",
        "            preds[X[:, self.feature_index] > self.threshold] = -1\n",
        "        return preds\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# AdaBoost Classifier\n",
        "# -----------------------------\n",
        "class AdaBoost:\n",
        "    def __init__(self, n_estimators=50):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.stumps = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        # Convert y to {-1, 1}\n",
        "        y = np.where(y == 0, -1, y)  # in case labels start at 0\n",
        "\n",
        "        # Initialize uniform weights\n",
        "        w = np.full(n_samples, (1 / n_samples))\n",
        "\n",
        "        self.stumps = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            stump = DecisionStump()\n",
        "            min_error = float(\"inf\")\n",
        "\n",
        "            # Find best stump\n",
        "            for feature_i in range(n_features):\n",
        "                feature_values = X[:, feature_i]\n",
        "                thresholds = np.unique(feature_values)\n",
        "                for threshold in thresholds:\n",
        "                    for polarity in [1, -1]:\n",
        "                        preds = np.ones(n_samples)\n",
        "                        if polarity == 1:\n",
        "                            preds[feature_values <= threshold] = -1\n",
        "                        else:\n",
        "                            preds[feature_values > threshold] = -1\n",
        "\n",
        "                        error = np.sum(w[y != preds])\n",
        "\n",
        "                        if error < min_error:\n",
        "                            stump.polarity = polarity\n",
        "                            stump.threshold = threshold\n",
        "                            stump.feature_index = feature_i\n",
        "                            min_error = error\n",
        "\n",
        "            # Compute alpha (learner weight)\n",
        "            EPS = 1e-10\n",
        "            stump.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
        "\n",
        "            # Update sample weights\n",
        "            preds = stump.predict(X)\n",
        "            w *= np.exp(-stump.alpha * y * preds)\n",
        "            w /= np.sum(w)  # normalize\n",
        "\n",
        "            self.stumps.append(stump)\n",
        "\n",
        "    def predict(self, X):\n",
        "        stump_preds = np.array([stump.alpha * stump.predict(X) for stump in self.stumps])\n",
        "        y_pred = np.sign(np.sum(stump_preds, axis=0))\n",
        "        return np.where(y_pred == -1, 0, y_pred)  # convert back {0,1,...}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Binary classification: Iris-setosa vs non-setosa\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = (iris.target != 0).astype(int)  # Setosa=0, others=1\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    clf = AdaBoost(n_estimators=20)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucPRGJAS8mVV",
        "outputId": "84f80bd8-02f3-4681-be5d-1ac486ea60fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "Confusion Matrix:\n",
            " [[15  0]\n",
            " [ 0 30]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voting Classifier"
      ],
      "metadata": {
        "id": "JYjF4qaz7p0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Voting Classifier Example with Scikit-learn\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Base models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define base learners\n",
        "log_clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "dt_clf = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "\n",
        "# --- Hard Voting ---\n",
        "hard_voting_clf = VotingClassifier(\n",
        "    estimators=[(\"lr\", log_clf), (\"knn\", knn_clf), (\"dt\", dt_clf)],\n",
        "    voting=\"hard\"\n",
        ")\n",
        "\n",
        "# --- Soft Voting ---\n",
        "soft_voting_clf = VotingClassifier(\n",
        "    estimators=[(\"lr\", log_clf), (\"knn\", knn_clf), (\"dt\", dt_clf)],\n",
        "    voting=\"soft\"\n",
        ")\n",
        "\n",
        "# Train and evaluate both\n",
        "for clf, label in [(hard_voting_clf, \"Hard Voting\"),\n",
        "                   (soft_voting_clf, \"Soft Voting\")]:\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(\"=\"*40)\n",
        "    print(label)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "5da-ZEmGCqKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64496ea-2fbb-447c-93e9-a3d248830e98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "Hard Voting\n",
            "Accuracy: 0.9555555555555556\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       0.93      0.93      0.93        15\n",
            "           2       0.93      0.93      0.93        15\n",
            "\n",
            "    accuracy                           0.96        45\n",
            "   macro avg       0.96      0.96      0.96        45\n",
            "weighted avg       0.96      0.96      0.96        45\n",
            "\n",
            "Confusion Matrix:\n",
            " [[15  0  0]\n",
            " [ 0 14  1]\n",
            " [ 0  1 14]]\n",
            "========================================\n",
            "Soft Voting\n",
            "Accuracy: 0.9333333333333333\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       0.93      0.87      0.90        15\n",
            "           2       0.88      0.93      0.90        15\n",
            "\n",
            "    accuracy                           0.93        45\n",
            "   macro avg       0.93      0.93      0.93        45\n",
            "weighted avg       0.93      0.93      0.93        45\n",
            "\n",
            "Confusion Matrix:\n",
            " [[15  0  0]\n",
            " [ 0 13  2]\n",
            " [ 0  1 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uGLbLftB7mZg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}