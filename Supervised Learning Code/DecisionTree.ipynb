{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Information Gain"
      ],
      "metadata": {
        "id": "S0M0BoZq36Eq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MIAGeSkx3-V",
        "outputId": "d94058e8-2c38-4a2b-fd41-0d4aa43f990e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Example 1: Categorical Data (Golf Play) ---\n",
            "Initial Entropy of Golf dataset: 0.9403\n",
            "Information Gain for 'Outlook': 0.2467\n",
            "Information Gain for 'Temperature': 0.0292\n",
            "Information Gain for 'Humidity': 0.1518\n",
            "Information Gain for 'Windy': 0.0481\n",
            "Best feature for Golf: 'Outlook' with Information Gain: 0.2467\n",
            "\n",
            "--- Example 2: Numerical Data (Disease Prediction) ---\n",
            "Initial Entropy of Medical dataset: 0.9710\n",
            "Information Gain for 'Age': 0.9710\n",
            "Information Gain for 'BloodPressure': 0.6100\n",
            "Best feature for Medical: 'Age' with Information Gain: 0.9710\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_entropy(data_subset: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the entropy for a subset of data based on the 'target' column.\n",
        "\n",
        "    Parameters:\n",
        "    data_subset (pd.DataFrame): A subset of the dataset that includes the 'target' column.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated entropy value.\n",
        "    \"\"\"\n",
        "    if data_subset.empty:\n",
        "        return 0.0\n",
        "\n",
        "    # Assume the last column or a column named 'target' is the class label.\n",
        "    # We prioritize 'target' if it exists for clarity.\n",
        "    target_column = data_subset.iloc[:, -1] if 'target' not in data_subset.columns else data_subset['target']\n",
        "\n",
        "    # Calculate the frequency of each class\n",
        "    class_counts = target_column.value_counts()\n",
        "    total_samples = len(target_column)\n",
        "\n",
        "    entropy = 0.0\n",
        "    for count in class_counts:\n",
        "        probability = count / total_samples\n",
        "        # Avoid log(0) for probabilities of 0\n",
        "        if probability > 0:\n",
        "            entropy -= probability * np.log2(probability)\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(data: pd.DataFrame, feature_name: str, target_name: str = 'target') -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Information Gain for a given feature.\n",
        "\n",
        "    Parameters:\n",
        "    data (pd.DataFrame): The entire dataset.\n",
        "    feature_name (str): The name of the feature for which to calculate Information Gain.\n",
        "    target_name (str): The name of the target (class) column. Defaults to 'target'.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated Information Gain value.\n",
        "    \"\"\"\n",
        "    # Entropy of the entire dataset\n",
        "    total_entropy = calculate_entropy(data)\n",
        "\n",
        "    # Get unique values for the feature\n",
        "    feature_values = data[feature_name].unique()\n",
        "\n",
        "    weighted_entropy = 0.0\n",
        "\n",
        "    # Check if the feature is categorical (object/category dtype) or numerical\n",
        "    if data[feature_name].dtype == 'object' or isinstance(data[feature_name].dtype, pd.CategoricalDtype):\n",
        "        # Categorical Feature\n",
        "        for value in feature_values:\n",
        "            subset = data[data[feature_name] == value]\n",
        "            weight = len(subset) / len(data)\n",
        "            weighted_entropy += weight * calculate_entropy(subset)\n",
        "    else:\n",
        "        # Numerical/Continuous Feature\n",
        "        # For continuous features, we need to find the best splitting threshold.\n",
        "        # Candidate thresholds are midpoints between sorted unique values.\n",
        "        sorted_values = sorted(feature_values)\n",
        "        candidate_thresholds = [(sorted_values[i] + sorted_values[i+1]) / 2\n",
        "                                for i in range(len(sorted_values) - 1)]\n",
        "\n",
        "        best_split_entropy = float('inf') # Initialize with infinity to find the minimum\n",
        "\n",
        "        if not candidate_thresholds: # If only one unique value for continuous feature\n",
        "            return 0.0 # Cannot split, information gain is 0\n",
        "\n",
        "        for threshold in candidate_thresholds:\n",
        "            subset_left = data[data[feature_name] <= threshold]\n",
        "            subset_right = data[data[feature_name] > threshold]\n",
        "\n",
        "            if not subset_left.empty and not subset_right.empty:\n",
        "                weight_left = len(subset_left) / len(data)\n",
        "                weight_right = len(subset_right) / len(data)\n",
        "                current_split_entropy = (weight_left * calculate_entropy(subset_left) +\n",
        "                                         weight_right * calculate_entropy(subset_right))\n",
        "                if current_split_entropy < best_split_entropy:\n",
        "                    best_split_entropy = current_split_entropy\n",
        "            elif subset_left.empty or subset_right.empty: # Avoids splits that result in empty subsets for meaningful IG\n",
        "                 current_split_entropy = float('inf') # Penalize invalid splits\n",
        "\n",
        "            if current_split_entropy < best_split_entropy:\n",
        "                    best_split_entropy = current_split_entropy\n",
        "\n",
        "        weighted_entropy = best_split_entropy # Use the best weighted entropy found\n",
        "\n",
        "    information_gain = total_entropy - weighted_entropy\n",
        "    return information_gain\n",
        "\n",
        "def find_best_attribute(data: pd.DataFrame, target_name: str = 'target') -> tuple[str | None, float]:\n",
        "    \"\"\"\n",
        "    Finds the best feature for splitting using Information Gain.\n",
        "\n",
        "    Parameters:\n",
        "    data (pd.DataFrame): The entire dataset.\n",
        "    target_name (str): The name of the target (class) column.\n",
        "\n",
        "    Returns:\n",
        "    tuple: (Name of the best feature, its Information Gain value).\n",
        "           Returns (None, 0.0) if no features are found or cannot be split.\n",
        "    \"\"\"\n",
        "    features = [col for col in data.columns if col != target_name]\n",
        "    if not features:\n",
        "        return None, 0.0\n",
        "\n",
        "    best_ig = -1.0 # Initialize to a value lower than any possible IG\n",
        "    best_feature = None\n",
        "\n",
        "    for feature in features:\n",
        "        current_ig = calculate_information_gain(data, feature, target_name)\n",
        "        if current_ig > best_ig:\n",
        "            best_ig = current_ig\n",
        "            best_feature = feature\n",
        "    return best_feature, best_ig\n",
        "\n",
        "# --- Usage Examples ---\n",
        "\n",
        "# 1. Example with Categorical Data (Golf Play Classifier - famous example)\n",
        "print(\"--- Example 1: Categorical Data (Golf Play) ---\")\n",
        "data_golf = pd.DataFrame({\n",
        "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
        "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
        "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
        "    'Windy': ['False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True'],\n",
        "    'PlayGolf': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
        "})\n",
        "# Rename the target column to 'target' for consistency\n",
        "data_golf = data_golf.rename(columns={'PlayGolf': 'target'})\n",
        "\n",
        "\n",
        "# Calculate initial entropy for the golf dataset\n",
        "entropy_golf = calculate_entropy(data_golf)\n",
        "print(f\"Initial Entropy of Golf dataset: {entropy_golf:.4f}\")\n",
        "\n",
        "# Calculate Information Gain for each feature\n",
        "features_golf = ['Outlook', 'Temperature', 'Humidity', 'Windy']\n",
        "for feature in features_golf:\n",
        "    ig = calculate_information_gain(data_golf, feature)\n",
        "    print(f\"Information Gain for '{feature}': {ig:.4f}\")\n",
        "\n",
        "# Find the best feature for golf\n",
        "best_feature_golf, best_ig_golf = find_best_attribute(data_golf)\n",
        "print(f\"Best feature for Golf: '{best_feature_golf}' with Information Gain: {best_ig_golf:.4f}\\n\")\n",
        "\n",
        "\n",
        "# 2. Example with Numerical Data (Disease Prediction with Age and Blood Pressure)\n",
        "print(\"--- Example 2: Numerical Data (Disease Prediction) ---\")\n",
        "data_medical = pd.DataFrame({\n",
        "    'Age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70],\n",
        "    'BloodPressure': [120, 130, 140, 125, 135, 150, 160, 145, 155, 165],\n",
        "    'target': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1] # 0: Healthy, 1: Diseased\n",
        "})\n",
        "\n",
        "# Calculate initial entropy for the medical dataset\n",
        "entropy_medical = calculate_entropy(data_medical)\n",
        "print(f\"Initial Entropy of Medical dataset: {entropy_medical:.4f}\")\n",
        "\n",
        "# Calculate Information Gain for each feature\n",
        "features_medical = ['Age', 'BloodPressure']\n",
        "for feature in features_medical:\n",
        "    ig = calculate_information_gain(data_medical, feature)\n",
        "    print(f\"Information Gain for '{feature}': {ig:.4f}\")\n",
        "\n",
        "# Find the best feature for medical data\n",
        "best_feature_medical, best_ig_medical = find_best_attribute(data_medical)\n",
        "print(f\"Best feature for Medical: '{best_feature_medical}' with Information Gain: {best_ig_medical:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gini"
      ],
      "metadata": {
        "id": "o5aspcvm34Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_gini_impurity(data_subset: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Gini Impurity for a subset of data based on the 'target' column.\n",
        "\n",
        "    Parameters:\n",
        "    data_subset (pd.DataFrame): A subset of the dataset that includes the 'target' column.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated Gini Impurity value.\n",
        "    \"\"\"\n",
        "    if data_subset.empty:\n",
        "        return 0.0\n",
        "\n",
        "    # Assume the last column or a column named 'target' is the class label.\n",
        "    target_column = data_subset.iloc[:, -1] if 'target' not in data_subset.columns else data_subset['target']\n",
        "\n",
        "    class_counts = target_column.value_counts()\n",
        "    total_samples = len(target_column)\n",
        "\n",
        "    gini_impurity = 1.0\n",
        "    for count in class_counts:\n",
        "        probability = count / total_samples\n",
        "        gini_impurity -= probability**2\n",
        "    return gini_impurity\n",
        "\n",
        "def calculate_gini_gain(data: pd.DataFrame, feature_name: str, target_name: str = 'target') -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Gini Gain (reduction in Gini Impurity) for a given feature.\n",
        "\n",
        "    Parameters:\n",
        "    data (pd.DataFrame): The entire dataset.\n",
        "    feature_name (str): The name of the feature for which to calculate Gini Gain.\n",
        "    target_name (str): The name of the target (class) column. Defaults to 'target'.\n",
        "\n",
        "    Returns:\n",
        "    float: The calculated Gini Gain value.\n",
        "    \"\"\"\n",
        "    # Gini Impurity of the entire dataset before splitting\n",
        "    total_gini_impurity = calculate_gini_impurity(data)\n",
        "\n",
        "    feature_values = data[feature_name].unique()\n",
        "\n",
        "    weighted_gini_impurity_after_split = 0.0\n",
        "\n",
        "    if data[feature_name].dtype == 'object' or isinstance(data[feature_name].dtype, pd.CategoricalDtype):\n",
        "        # Categorical Feature\n",
        "        for value in feature_values:\n",
        "            subset = data[data[feature_name] == value]\n",
        "            weight = len(subset) / len(data)\n",
        "            weighted_gini_impurity_after_split += weight * calculate_gini_impurity(subset)\n",
        "    else:\n",
        "        # Numerical/Continuous Feature\n",
        "        # Find the best splitting threshold\n",
        "        sorted_values = sorted(feature_values)\n",
        "        candidate_thresholds = [(sorted_values[i] + sorted_values[i+1]) / 2\n",
        "                                for i in range(len(sorted_values) - 1)]\n",
        "\n",
        "        best_split_gini = float('inf') # Initialize with infinity to find the minimum weighted Gini\n",
        "\n",
        "        if not candidate_thresholds: # If only one unique value for continuous feature\n",
        "            return 0.0 # Cannot split, Gini Gain is 0\n",
        "\n",
        "        for threshold in candidate_thresholds:\n",
        "            subset_left = data[data[feature_name] <= threshold]\n",
        "            subset_right = data[data[feature_name] > threshold]\n",
        "\n",
        "            if not subset_left.empty and not subset_right.empty:\n",
        "                weight_left = len(subset_left) / len(data)\n",
        "                weight_right = len(subset_right) / len(data)\n",
        "                current_split_gini = (weight_left * calculate_gini_impurity(subset_left) +\n",
        "                                      weight_right * calculate_gini_impurity(subset_right))\n",
        "\n",
        "                if current_split_gini < best_split_gini:\n",
        "                    best_split_gini = current_split_gini\n",
        "            elif subset_left.empty or subset_right.empty:\n",
        "                # Penalize splits that result in empty subsets, making them less likely to be chosen.\n",
        "                current_split_gini = float('inf')\n",
        "\n",
        "            # Update best_split_gini even if it's inf from a penalized split, as long as it's the current best\n",
        "            if current_split_gini < best_split_gini:\n",
        "                best_split_gini = current_split_gini\n",
        "\n",
        "        weighted_gini_impurity_after_split = best_split_gini # Use the best weighted Gini found\n",
        "\n",
        "    gini_gain = total_gini_impurity - weighted_gini_impurity_after_split\n",
        "    return gini_gain\n",
        "\n",
        "def find_best_attribute_gini(data: pd.DataFrame, target_name: str = 'target') -> tuple[str | None, float]:\n",
        "    \"\"\"\n",
        "    Finds the best feature for splitting using Gini Gain.\n",
        "\n",
        "    Parameters:\n",
        "    data (pd.DataFrame): The entire dataset.\n",
        "    target_name (str): The name of the target (class) column.\n",
        "\n",
        "    Returns:\n",
        "    tuple: (Name of the best feature, its Gini Gain value).\n",
        "           Returns (None, 0.0) if no features are found or cannot be split meaningfully.\n",
        "    \"\"\"\n",
        "    features = [col for col in data.columns if col != target_name]\n",
        "    if not features:\n",
        "        return None, 0.0\n",
        "\n",
        "    best_gini_gain = -1.0 # Initialize to a value lower than any possible Gini Gain\n",
        "    best_feature = None\n",
        "\n",
        "    for feature in features:\n",
        "        current_gini_gain = calculate_gini_gain(data, feature, target_name)\n",
        "        if current_gini_gain > best_gini_gain:\n",
        "            best_gini_gain = current_gini_gain\n",
        "            best_feature = feature\n",
        "    return best_feature, best_gini_gain\n",
        "\n",
        "# --- Usage Examples ---\n",
        "\n",
        "# 1. Example with Categorical Data (Golf Play Classifier - famous example)\n",
        "print(\"--- Example 1: Categorical Data (Golf Play) ---\")\n",
        "data_golf = pd.DataFrame({\n",
        "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
        "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
        "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
        "    'Windy': ['False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True'],\n",
        "    'PlayGolf': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
        "})\n",
        "# Rename the target column to 'target' for consistency\n",
        "data_golf = data_golf.rename(columns={'PlayGolf': 'target'})\n",
        "\n",
        "\n",
        "# Calculate initial Gini Impurity for the golf dataset\n",
        "gini_golf = calculate_gini_impurity(data_golf)\n",
        "print(f\"Initial Gini Impurity of Golf dataset: {gini_golf:.4f}\")\n",
        "\n",
        "# Calculate Gini Gain for each feature\n",
        "features_golf = ['Outlook', 'Temperature', 'Humidity', 'Windy']\n",
        "for feature in features_golf:\n",
        "    gg = calculate_gini_gain(data_golf, feature)\n",
        "    print(f\"Gini Gain for '{feature}': {gg:.4f}\")\n",
        "\n",
        "# Find the best feature for golf using Gini Gain\n",
        "best_feature_golf_gini, best_gg_golf = find_best_attribute_gini(data_golf)\n",
        "print(f\"Best feature for Golf (Gini): '{best_feature_golf_gini}' with Gini Gain: {best_gg_golf:.4f}\\n\")\n",
        "\n",
        "\n",
        "# 2. Example with Numerical Data (Disease Prediction with Age and Blood Pressure)\n",
        "print(\"--- Example 2: Numerical Data (Disease Prediction) ---\")\n",
        "data_medical = pd.DataFrame({\n",
        "    'Age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70],\n",
        "    'BloodPressure': [120, 130, 140, 125, 135, 150, 160, 145, 155, 165],\n",
        "    'target': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1] # 0: Healthy, 1: Diseased\n",
        "})\n",
        "\n",
        "# Calculate initial Gini Impurity for the medical dataset\n",
        "gini_medical = calculate_gini_impurity(data_medical)\n",
        "print(f\"Initial Gini Impurity of Medical dataset: {gini_medical:.4f}\")\n",
        "\n",
        "# Calculate Gini Gain for each feature\n",
        "features_medical = ['Age', 'BloodPressure']\n",
        "for feature in features_medical:\n",
        "    gg = calculate_gini_gain(data_medical, feature)\n",
        "    print(f\"Gini Gain for '{feature}': {gg:.4f}\")\n",
        "\n",
        "# Find the best feature for medical data using Gini Gain\n",
        "best_feature_medical_gini, best_gg_medical = find_best_attribute_gini(data_medical)\n",
        "print(f\"Best feature for Medical (Gini): '{best_feature_medical_gini}' with Gini Gain: {best_gg_medical:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSfbSvr4x-6G",
        "outputId": "bd495117-2963-4213-cfa5-6efc056f3ff5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Example 1: Categorical Data (Golf Play) ---\n",
            "Initial Gini Impurity of Golf dataset: 0.4592\n",
            "Gini Gain for 'Outlook': 0.1163\n",
            "Gini Gain for 'Temperature': 0.0187\n",
            "Gini Gain for 'Humidity': 0.0918\n",
            "Gini Gain for 'Windy': 0.0306\n",
            "Best feature for Golf (Gini): 'Outlook' with Gini Gain: 0.1163\n",
            "\n",
            "--- Example 2: Numerical Data (Disease Prediction) ---\n",
            "Initial Gini Impurity of Medical dataset: 0.4800\n",
            "Gini Gain for 'Age': 0.4800\n",
            "Gini Gain for 'BloodPressure': 0.3200\n",
            "Best feature for Medical (Gini): 'Age' with Gini Gain: 0.4800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "ju7q4SMD303S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import graphviz\n",
        "\n",
        "\n",
        "\n",
        "data_golf_sklearn = pd.DataFrame({\n",
        "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
        "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
        "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
        "    'Windy': ['False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True'],\n",
        "    'PlayGolf': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
        "})\n",
        "\n",
        "le_outlook = LabelEncoder()\n",
        "le_temp = LabelEncoder()\n",
        "le_humidity = LabelEncoder()\n",
        "le_windy = LabelEncoder()\n",
        "le_playgolf = LabelEncoder()\n",
        "\n",
        "data_golf_sklearn['Outlook_encoded'] = le_outlook.fit_transform(data_golf_sklearn['Outlook'])\n",
        "data_golf_sklearn['Temperature_encoded'] = le_temp.fit_transform(data_golf_sklearn['Temperature'])\n",
        "data_golf_sklearn['Humidity_encoded'] = le_humidity.fit_transform(data_golf_sklearn['Humidity'])\n",
        "data_golf_sklearn['Windy_encoded'] = le_windy.fit_transform(data_golf_sklearn['Windy'])\n",
        "data_golf_sklearn['PlayGolf_encoded'] = le_playgolf.fit_transform(data_golf_sklearn['PlayGolf'])\n",
        "\n",
        "X_sklearn = data_golf_sklearn[['Outlook_encoded', 'Temperature_encoded', 'Humidity_encoded', 'Windy_encoded']]\n",
        "y_sklearn = data_golf_sklearn['PlayGolf_encoded']\n",
        "\n",
        "feature_names_sklearn = ['Outlook', 'Temperature', 'Humidity', 'Windy']\n",
        "class_names_sklearn = le_playgolf.classes_.tolist() # ['No', 'Yes']\n",
        "\n",
        "print(\"\\n--- Scikit-learn Decision Tree ---\")\n",
        "\n",
        "\n",
        "dt_sklearn = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)\n",
        "dt_sklearn.fit(X_sklearn, y_sklearn)\n",
        "\n",
        "predictions_sklearn = dt_sklearn.predict(X_sklearn)\n",
        "accuracy_sklearn = accuracy_score(y_sklearn, predictions_sklearn)\n",
        "print(f\"Scikit-learn Decision Tree Training Accuracy: {accuracy_sklearn * 100:.2f}%\")\n",
        "\n",
        "print(\"\\n--- Plotting Scikit-learn Decision Tree ---\")\n",
        "\n",
        "dot_data = export_graphviz(dt_sklearn,\n",
        "                           out_file=None,\n",
        "                           feature_names=feature_names_sklearn,\n",
        "                           class_names=class_names_sklearn,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "iYNGTdI12slj",
        "outputId": "254d58a4-94fc-4888-9651-516c3649366e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Scikit-learn Decision Tree ---\n",
            "Scikit-learn Decision Tree Training Accuracy: 78.57%\n",
            "\n",
            "--- Plotting Scikit-learn Decision Tree ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"465pt\" height=\"433pt\"\n viewBox=\"0.00 0.00 465.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-429 461,-429 461,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#a7d3f3\" stroke=\"black\" d=\"M205,-425C205,-425 125,-425 125,-425 119,-425 113,-419 113,-413 113,-413 113,-354 113,-354 113,-348 119,-342 125,-342 125,-342 205,-342 205,-342 211,-342 217,-348 217,-354 217,-354 217,-413 217,-413 217,-419 211,-425 205,-425\"/>\n<text text-anchor=\"start\" x=\"124\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Outlook ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"121\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.94</text>\n<text text-anchor=\"start\" x=\"124\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 14</text>\n<text text-anchor=\"start\" x=\"125.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 9]</text>\n<text text-anchor=\"start\" x=\"130\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Yes</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#399de5\" stroke=\"black\" d=\"M142,-298.5C142,-298.5 70,-298.5 70,-298.5 64,-298.5 58,-292.5 58,-286.5 58,-286.5 58,-242.5 58,-242.5 58,-236.5 64,-230.5 70,-230.5 70,-230.5 142,-230.5 142,-230.5 148,-230.5 154,-236.5 154,-242.5 154,-242.5 154,-286.5 154,-286.5 154,-292.5 148,-298.5 142,-298.5\"/>\n<text text-anchor=\"start\" x=\"66\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"68.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n<text text-anchor=\"start\" x=\"66.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4]</text>\n<text text-anchor=\"start\" x=\"71\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Yes</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M144.53,-341.91C138.97,-330.87 132.93,-318.9 127.32,-307.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"130.35,-306.02 122.73,-298.67 124.1,-309.17 130.35,-306.02\"/>\n<text text-anchor=\"middle\" x=\"114.9\" y=\"-318.71\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M264,-306C264,-306 184,-306 184,-306 178,-306 172,-300 172,-294 172,-294 172,-235 172,-235 172,-229 178,-223 184,-223 184,-223 264,-223 264,-223 270,-223 276,-229 276,-235 276,-235 276,-294 276,-294 276,-300 270,-306 264,-306\"/>\n<text text-anchor=\"start\" x=\"180\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Humidity ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"184\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.0</text>\n<text text-anchor=\"start\" x=\"183\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\n<text text-anchor=\"start\" x=\"184.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5, 5]</text>\n<text text-anchor=\"start\" x=\"192\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = No</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M185.47,-341.91C189.82,-333.29 194.45,-324.09 198.95,-315.17\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"202.19,-316.53 203.57,-306.02 195.94,-313.37 202.19,-316.53\"/>\n<text text-anchor=\"middle\" x=\"211.39\" y=\"-326.06\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#eeab7b\" stroke=\"black\" d=\"M203.5,-187C203.5,-187 116.5,-187 116.5,-187 110.5,-187 104.5,-181 104.5,-175 104.5,-175 104.5,-116 104.5,-116 104.5,-110 110.5,-104 116.5,-104 116.5,-104 203.5,-104 203.5,-104 209.5,-104 215.5,-110 215.5,-116 215.5,-116 215.5,-175 215.5,-175 215.5,-181 209.5,-187 203.5,-187\"/>\n<text text-anchor=\"start\" x=\"123.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Windy ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"112.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.811</text>\n<text text-anchor=\"start\" x=\"122.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n<text text-anchor=\"start\" x=\"120.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [3, 1]</text>\n<text text-anchor=\"start\" x=\"128\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = No</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M201.8,-222.91C197.03,-214.2 191.94,-204.9 187.02,-195.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"190.03,-194.11 182.17,-187.02 183.89,-197.47 190.03,-194.11\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#9ccef2\" stroke=\"black\" d=\"M332.5,-187C332.5,-187 245.5,-187 245.5,-187 239.5,-187 233.5,-181 233.5,-175 233.5,-175 233.5,-116 233.5,-116 233.5,-110 239.5,-104 245.5,-104 245.5,-104 332.5,-104 332.5,-104 338.5,-104 344.5,-110 344.5,-116 344.5,-116 344.5,-175 344.5,-175 344.5,-181 338.5,-187 332.5,-187\"/>\n<text text-anchor=\"start\" x=\"252.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Windy ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"241.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.918</text>\n<text text-anchor=\"start\" x=\"251.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"249.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2, 4]</text>\n<text text-anchor=\"start\" x=\"254\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Yes</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.55,-222.91C251.39,-214.2 256.56,-204.9 261.56,-195.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"264.69,-197.46 266.49,-187.02 258.57,-194.06 264.69,-197.46\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M84,-68C84,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 84,0 84,0 90,0 96,-6 96,-12 96,-12 96,-56 96,-56 96,-62 90,-68 84,-68\"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.0</text>\n<text text-anchor=\"start\" x=\"10.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"8.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 1]</text>\n<text text-anchor=\"start\" x=\"16\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = No</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M118.3,-103.73C108.87,-94.51 98.87,-84.74 89.46,-75.53\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"91.66,-72.79 82.06,-68.3 86.76,-77.79 91.66,-72.79\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M198,-68C198,-68 126,-68 126,-68 120,-68 114,-62 114,-56 114,-56 114,-12 114,-12 114,-6 120,0 126,0 126,0 198,0 198,0 204,0 210,-6 210,-12 210,-12 210,-56 210,-56 210,-62 204,-68 198,-68\"/>\n<text text-anchor=\"start\" x=\"122\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"124.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"122.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2, 0]</text>\n<text text-anchor=\"start\" x=\"130\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = No</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M160.74,-103.73C160.89,-95.52 161.05,-86.86 161.2,-78.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"164.71,-78.36 161.39,-68.3 157.71,-78.23 164.71,-78.36\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#7bbeee\" stroke=\"black\" d=\"M330.5,-68C330.5,-68 243.5,-68 243.5,-68 237.5,-68 231.5,-62 231.5,-56 231.5,-56 231.5,-12 231.5,-12 231.5,-6 237.5,0 243.5,0 243.5,0 330.5,0 330.5,0 336.5,0 342.5,-6 342.5,-12 342.5,-12 342.5,-56 342.5,-56 342.5,-62 336.5,-68 330.5,-68\"/>\n<text text-anchor=\"start\" x=\"239.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.811</text>\n<text text-anchor=\"start\" x=\"249.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n<text text-anchor=\"start\" x=\"247.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 3]</text>\n<text text-anchor=\"start\" x=\"252\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = Yes</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M288.26,-103.73C288.11,-95.52 287.95,-86.86 287.8,-78.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"291.29,-78.23 287.61,-68.3 284.29,-78.36 291.29,-78.23\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M445,-68C445,-68 373,-68 373,-68 367,-68 361,-62 361,-56 361,-56 361,-12 361,-12 361,-6 367,0 373,0 373,0 445,0 445,0 451,0 457,-6 457,-12 457,-12 457,-56 457,-56 457,-62 451,-68 445,-68\"/>\n<text text-anchor=\"start\" x=\"369\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.0</text>\n<text text-anchor=\"start\" x=\"371.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"369.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 1]</text>\n<text text-anchor=\"start\" x=\"377\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = No</text>\n</g>\n<!-- 6&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>6&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M333.68,-103.73C343.88,-94.42 354.71,-84.54 364.88,-75.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"367.48,-77.63 372.51,-68.3 362.76,-72.45 367.48,-77.63\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7bfd65fcebd0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}